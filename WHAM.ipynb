{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module and class setup.\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def flat_fes(x_vals):\n",
    "    fes = np.zeros_like(x_vals)\n",
    "    fes -= fes.min()\n",
    "    return fes\n",
    "\n",
    "def shallow_fes(x_vals):\n",
    "    fes = x_vals**4 - 0.85*x_vals**2\n",
    "    fes -= fes.min()\n",
    "    return fes\n",
    "\n",
    "def deep_fes(x_vals):\n",
    "    fes = 100*x_vals**4 - 100*x_vals**2\n",
    "    fes -= fes.min()\n",
    "    return fes\n",
    "\n",
    "def harmonic_restraint(x_vals,center,kappa):\n",
    "        return 0.5 * kappa * (x_vals - center)**2\n",
    "\n",
    "class Dynamics():\n",
    "    def __init__(self, fes_func):\n",
    "\n",
    "        self.x_vals = np.linspace(-1,1,100)\n",
    "        self.bin_width = self.x_vals[1] - self.x_vals[0]\n",
    "        self.fes_func = fes_func\n",
    "        self.underlying_fes = fes_func(self.x_vals)\n",
    "        \n",
    "        self.boltz_weights = np.exp(-self.underlying_fes)\n",
    "\n",
    "        self.underlying_prob_dist = self.boltz_weights / (np.sum(self.boltz_weights)*self.bin_width)\n",
    "\n",
    "    def dynamics(self,nsteps):\n",
    "        x_index = np.where(self.underlying_fes == self.underlying_fes.min())[0][0]\n",
    "        coord = self.x_vals[x_index]\n",
    "        self.time = [0]\n",
    "        self.trajectory = [coord]\n",
    "        num_acceptances = 0\n",
    "\n",
    "        for i in range(nsteps):\n",
    "            rand = np.random.rand()\n",
    "            self.time.append(i+1)\n",
    "            \n",
    "            if rand >= 0.5:\n",
    "                # Impose PBC\n",
    "                if (x_index + 1) == 100:\n",
    "                    trial_index = 0\n",
    "\n",
    "                else:\n",
    "                    trial_index = x_index + 1\n",
    "\n",
    "                trans_prob = self.boltz_weights[trial_index] / self.boltz_weights[x_index]\n",
    "                if trans_prob >= np.random.rand():\n",
    "                    x_index = trial_index\n",
    "                    num_acceptances += 1\n",
    "\n",
    "            else:\n",
    "                # Impose PBC\n",
    "                if (x_index - 1) == -1:\n",
    "                    trial_index = 99\n",
    "\n",
    "                else:\n",
    "                    trial_index = x_index - 1\n",
    "\n",
    "                trans_prob = self.boltz_weights[trial_index] / self.boltz_weights[x_index]\n",
    "                if trans_prob >= np.random.rand():\n",
    "                    x_index = trial_index\n",
    "                    num_acceptances += 1\n",
    "\n",
    "            coord = self.x_vals[x_index]\n",
    "            \n",
    "            self.trajectory.append(coord)\n",
    "        \n",
    "        self.acceptance_prob = num_acceptances / nsteps\n",
    "\n",
    "    def calc_unbiased_prob(self):\n",
    "        hist, _ = np.histogram(self.trajectory,range=(-1,1),bins=100)\n",
    "\n",
    "        self.prob_hist = hist/(hist.sum()*self.bin_width)\n",
    "        weights = np.exp(self.bias,dtype='float128')\n",
    "        self.unbiased_prob = self.prob_hist*weights\n",
    "\n",
    "class Umbrella_sampling():\n",
    "    def __init__(self,numbrellas,kappa,fes_func):\n",
    "\n",
    "        self.numbrellas = numbrellas\n",
    "        self.centers = np.linspace(-1,1,numbrellas)\n",
    "        self.replicas = []\n",
    "        for i in range(numbrellas):\n",
    "            replica_window = Dynamics(fes_func)\n",
    "            replica_window.bias = harmonic_restraint((replica_window.x_vals),(self.centers[i]),kappa)\n",
    "            replica_window.underlying_fes += replica_window.bias\n",
    "            replica_window.boltz_weights = np.exp(-replica_window.underlying_fes)\n",
    "            replica_window.underlying_prob_dist = replica_window.boltz_weights / (np.sum(replica_window.boltz_weights)*replica_window.bin_width)\n",
    "            self.replicas.append(copy.deepcopy(replica_window))\n",
    "\n",
    "    def dynamics(self,nsteps):\n",
    "        self.nsteps = nsteps\n",
    "        for i in range(self.numbrellas):\n",
    "            self.replicas[i].dynamics(nsteps)\n",
    "\n",
    "    \n",
    "class WHAM():\n",
    "    def __init__(self,umb_sim,iterations):\n",
    "\n",
    "        self.umb_sim = umb_sim\n",
    "        self.f = np.ones(umb_sim.numbrellas)\n",
    "        self.N = np.ones_like(self.f)*umb_sim.nsteps # This is incorrect, in principle, because the samples are correlated.\n",
    "        self.c = np.array([np.exp(-window.bias) for window in umb_sim.replicas])\n",
    "\n",
    "        n = []\n",
    "        for window in umb_sim.replicas:\n",
    "            hist, _ = np.histogram(window.trajectory,range=(-1,1),bins=100)\n",
    "            n.append(hist)\n",
    "        self.n = np.array(n)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            self.calc_p()\n",
    "            self.calc_f()\n",
    "\n",
    "        self.calc_G()\n",
    "\n",
    "    def calc_p(self):\n",
    "        numerator = np.sum(self.n,axis=0)\n",
    "        denominator = np.sum((self.N*self.f*self.c.T).T,axis=0)\n",
    "        self.p = numerator / denominator\n",
    "\n",
    "    def calc_f(self):\n",
    "        self.f = 1 / np.sum(self.c*self.p,axis=1)\n",
    "\n",
    "    def calc_G(self):\n",
    "        self.G = -np.log(self.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b486",
   "metadata": {},
   "source": [
    "## Free energy as a function of collective variables.\n",
    "\n",
    "As we have learned in class, the \"Free Energy\" of a given microstate of a system is really just a measurement of how likely that microstate is. There is a single simple equation that relates the free energy $F$ to the probability of a state $P$ (see Appendix I for the derivation):\n",
    "\n",
    "$$F(\\mathbf{q}) = -k_BT\\ln{P(\\mathbf{q})}$$\n",
    "\n",
    "where $\\mathbf{q}$ are the microscopic coordinates of the system (i.e. positions and momenta of particles), $T$ is the temperature in Kelvin, and $k_B$ is the Boltzmann constant with units of energy-per-Kelvin.\n",
    "\n",
    "Written as functions of $\\mathbf{q}$, $F$ and $P$ are $6N$-dimensional quantities, where $N$ is the number of particles in the classical system. This is seldom useful because of how impossible it becomes to sample such a high-dimensional phase space. Instead we pick some particular degrees of freedom that we are interested in from our simulation. Perhaps you are interested in seeing the distance between two residues in a protein, or the dipole moment for a group of atoms, or a specific dihedral angle in your molecule. The important thing is simply that the so-called \"collective variable\" of interest can be computed as a function of the microscopic coordinates $\\mathbf{q}$. From here, we can reduce the dimensionality of $P$ down to however few CVs $\\mathbf{s}$ we want using the following equation:\n",
    "\n",
    "$$P(\\mathbf{s}) = \\int{P(\\mathbf{q})\\delta(\\mathbf{s} - \\mathbf{s}(\\mathbf{q}))} \\text{d}\\mathbf{q}$$\n",
    "\n",
    "But the truth is that this equation does not have much practical utility for us; it only serves to justify collective variable-based methods. The important thing is the intuition that the free energy can be computed as a function of any CV we want, as long as the CV can be calculated from the microscopic coordinates.\n",
    "\n",
    "It follows naturally that the free energy can then also be described as a function of $\\mathbf{s}$:\n",
    "\n",
    "$$F(\\mathbf{s}) = -k_BT\\ln{{P(\\mathbf{s})}}$$\n",
    "\n",
    "(Note: If only one CV is used to describe the system, it is sometimes called the reaction coordinate, and $F$ and $P$ become one-dimensional functions. This will be the case for today's example.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0413df",
   "metadata": {},
   "source": [
    "## Exercise 1: Using samples from a simulation to estimate $P(s)$ and $F(s)$.\n",
    "\n",
    "Let's say we want to compute the free energy barrier of some transition that our system will undergo. The use of collective variables makes this, at least in principle, very simple! At every step in our simulation, we compute the value of the $s(\\mathbf{q})$ that we're interested in. We therefore record the trajectory of our simulation in CV space, and can bin those values into a histogram. If we normalize this histogram such that its total area is equal to 1, this serves as an estimate of $P(s)$.\n",
    "\n",
    "Let's try it! First, run the following cell to take a look at the underlying free energy that we'll have for our toy simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48361811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the underlying free energy of the simulation\n",
    "\n",
    "x_vals = np.linspace(-1,1,100)\n",
    "underlying_fes = shallow_fes(x_vals)\n",
    "plt.plot(x_vals,underlying_fes)\n",
    "plt.xlabel('CV')\n",
    "plt.ylabel('Free Energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c486e7",
   "metadata": {},
   "source": [
    "Now, in the following cell, we want to use the equation $F(s) = -k_BT \\ln (P(s))$ to compute the probability distribution that we should expect from the given free energy surface (FES). Figure out the code that must go in the blank in order to compute $P(s)$ properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32477493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, fill in the blank to compute the expected probability distribution from the underlying FES.\n",
    "# Hint: To compute exponentials, you can use the np.exp() function implemented in numpy.\n",
    "\n",
    "kBT = 1\n",
    "\n",
    "underlying_prob = # Your answer here\n",
    "underlying_prob /= np.sum(underlying_prob*0.02) # It is good practice to renormalize the probability distrubution to an area of 1\n",
    "\n",
    "plt.plot(x_vals,underlying_prob)\n",
    "plt.xlabel('CV')\n",
    "plt.ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3163fb67",
   "metadata": {},
   "source": [
    "Now that you know what we should expect to see, run the next cell to sample this free energy surface with a simple Metropolis Monte Carlo algorithm, and compare the theoretical probability distribution to the estimate we get from a simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea56f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the dynamics\n",
    "\n",
    "dyn = Dynamics(fes_func=shallow_fes)\n",
    "dyn.dynamics(10000000)\n",
    "\n",
    "# Plot results\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(dyn.time,dyn.trajectory)\n",
    "ax1.set_title('CV vs. Time')\n",
    "\n",
    "hist, _ = np.histogram(dyn.trajectory,range=(-1,1),bins=100,density=True)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(dyn.x_vals,underlying_prob,label='Underlying')\n",
    "ax2.plot(dyn.x_vals,hist,label='Dynamics Estimate')\n",
    "ax2.set_title('Probability Distribution')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074018c",
   "metadata": {},
   "source": [
    "Now let's practice the reverse. Take the histogram of CV values that was computed in the previous simulation and use it to compute the estimated FES from our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fill in the blank to compute the estimate of the free energy surface from our simulation.\n",
    "# Hint: Start by figuring out which variable from the previous cell represents the estimate of our probability distribution.\n",
    "\n",
    "fes = # Fill in blank\n",
    "\n",
    "plt.plot(x_vals,underlying_fes,label='Underlying')\n",
    "plt.plot(x_vals,fes-fes.min(),label='Dynamics Estimate')\n",
    "plt.xlabel('CV')\n",
    "plt.ylabel('Free Energy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1b545",
   "metadata": {},
   "source": [
    "## Exercise 2: Overcoming high free-energy barriers.\n",
    "\n",
    "Run the following cell to perform dynamics on an underlying free energy with a much higher barrier. What do you observe about the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = Dynamics(fes_func=deep_fes)\n",
    "dyn.dynamics(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results of previous cell.\n",
    "\n",
    "fig = plt.figure(figsize=(22.5,5))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.plot(dyn.time,dyn.trajectory)\n",
    "ax1.set_title('CV vs. Time')\n",
    "ax1.set_ylim(-1,1)\n",
    "\n",
    "hist, _ = np.histogram(dyn.trajectory,range=(-1,1),bins=100,density=True)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.plot(dyn.x_vals,dyn.underlying_prob_dist,label='Underlying')\n",
    "ax2.plot(dyn.x_vals,hist,label='Dynamics Estimate')\n",
    "ax2.set_title('Probability Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "fes = -np.log(hist)\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.plot(dyn.x_vals,dyn.underlying_fes - dyn.underlying_fes.min(),label='Underlying')\n",
    "ax3.plot(dyn.x_vals,fes-fes.min(),label='Dynamics Estimate')\n",
    "ax3.set_title('Free Energy')\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fdcd6a",
   "metadata": {},
   "source": [
    "You can see that the random walk cannot overcome the free energy barrier in order to sample both free-energy minima, let alone sample the transition state between the two. This is an extremely common problem with statistical sampling simulations. It would require an unfeasibly large number of steps (i.e., an extremely long computation time) in order to achieve a reasonable estimate for the underlying free energy of this system.\n",
    "\n",
    "Thankfully, there are a huge number of methods for enhancing the sampling rate of simulations. We will look at two of these today, the first known as \"umbrella sampling\", and the second being an extension of the umbrella sampling method known as the \"weighted histogram analysis method\", or \"WHAM\".\n",
    "\n",
    "The idea behind umbrella sampling is simple: to add a bias potential to your simulation that negates the barrier from the transition state. Let's run our dynamics again, but this time add a harmonic potential to our system to force transitions. A harmonic potential has 2 parameters: the center, which we will set to 0 since that is where the transition state lies, and kappa, which determines how steep the potential is. Try adjusting kappa below until the center of the free energy surface is fairly flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = 100 # Play with kappa until you find a value that flattens the free energy surface nicely!\n",
    "\n",
    "umb = Dynamics(fes_func=deep_fes)\n",
    "underlying_fes = umb.underlying_fes.copy()\n",
    "\n",
    "fig = plt.figure(figsize=(22.5,5))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.plot(umb.x_vals,underlying_fes)\n",
    "ax1.set_title('Underlying FES')\n",
    "\n",
    "bias_potential = harmonic_restraint(umb.x_vals,center=0,kappa=kappa)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.plot(umb.x_vals,bias_potential)\n",
    "ax2.set_title('Bias Potential')\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.plot(umb.x_vals,underlying_fes+bias_potential)\n",
    "ax3.set_title('Sum')\n",
    "\n",
    "umb.underlying_fes += bias_potential\n",
    "umb.boltz_weights = np.exp(-umb.underlying_fes)\n",
    "umb.underlying_prob_dist = umb.boltz_weights / (np.sum(umb.boltz_weights)*umb.bin_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ececfd9d",
   "metadata": {},
   "source": [
    "Once you are happy with the value of kappa you've chosen, run the dynamics again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b79fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "umb.dynamics(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44742a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results from previous cell\n",
    "\n",
    "fig = plt.figure(figsize=(22.5,5))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.plot(umb.time,umb.trajectory)\n",
    "ax1.set_title('CV vs. Time')\n",
    "ax1.set_ylim(-1,1)\n",
    "\n",
    "hist, _ = np.histogram(umb.trajectory,range=(-1,1),bins=100,density=True)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.plot(umb.x_vals,umb.underlying_prob_dist,label='Underlying + Bias Potential')\n",
    "ax2.plot(umb.x_vals,hist,label='Dynamics Estimate')\n",
    "ax2.set_title('Probability Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "fes = -np.log(hist)\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.plot(umb.x_vals,umb.underlying_fes - umb.underlying_fes.min(),label='Underlying + Bias Potential')\n",
    "ax3.plot(umb.x_vals,fes-fes.min(),label='Dynamics Estimate')\n",
    "ax3.set_title('Free Energy')\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e185f6ee",
   "metadata": {},
   "source": [
    "Assuming that you have chosen kappa well, your simulation should have sampled both free energy minima as well as the transition state. This likely came at the expense that you were not able to sample the edges of the box, which may or may not be fine in a real scenario, depending on the nature of the CV you've chosen for analysis.\n",
    "\n",
    "Our simulation converging to the the sum of the free energy and the bias potential is a good first result, but we want to calculate the actual free energy. Luckily, the concept behind reweighting our trajectories to recover the true free energy is quite simple.\n",
    "\n",
    "$$P_{b}(s) = e^{\\big(\\frac{-(F(s)+V(s))}{k_BT}\\big)}$$\n",
    "\n",
    "$$\\ln P_{b}(s) = \\bigg(\\frac{-(F(s)+V(s))}{k_BT}\\bigg)$$\n",
    "\n",
    "$$-k_BT \\ln P_{b}(s) = F(s)+V(s)$$\n",
    "\n",
    "$$-k_BT \\ln P_{b}(s) - V(s) = F(s)$$\n",
    "\n",
    "Where $P_{b}(s)$ is the biased probability distribution that we sampled and $V(s)$ is the bias potential we applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b1a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kBT = 1\n",
    "fes = -kBT*np.log(hist) - bias_potential\n",
    "\n",
    "plt.plot(umb.x_vals,underlying_fes-underlying_fes.min())\n",
    "plt.plot(umb.x_vals,fes-fes.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33dd403",
   "metadata": {},
   "source": [
    "## Exercise 3: Weighted Histogram Analysis Method\n",
    "\n",
    "The original implementation of umbrella sampling, as described above, has some significant limitations. Firstly, in a real scenario, you will obviously not know what the free energy looks like, so picking the exact right position and shape for the bias potential is nearly impossible. Secondly, as you can see, we actually increase the difficult of sampling different parts of CV space, which is not ideal.\n",
    "\n",
    "The extension of umbrella sampling that addresses these problems is known as the weighted-histogram analysis method, or WHAM. In fact, nowadays, when someone speaks about doing an umbrella sampling simulation, they are very likely actually referring to WHAM or a similar alternative.\n",
    "\n",
    "The brilliant innovation of WHAM is essential to run multiple shorter umbrella sampling simulations, each with the bias potential centered at a different point in CV space. This way, we force the simulation to sample every part of CV space, regardless of what the underlying free energy looks like. This may seem like an obvious thing to try, but it turns out that finding a way to stitch the results of each simulation together is not at all trivial. In 1992, Kumar et al. introduced the following equations to overcome this problem.\n",
    "\n",
    "$$f_i = \\frac{1}{\\sum_j p_j^\\circ c_{ij}}$$\n",
    "\n",
    "$$p_j^\\circ = \\frac{\\sum_{i=1}^{S}n_{ij}}{\\sum_{i=1}^{S}N_ic_{ij}f_i}$$\n",
    "\n",
    "Where $i$ indexes over $S$ simulations, each with their own bias potential, and $j$ indexes over the bins in each histogram. $p_j^\\circ$ is the unbiased probability in bin $j$, which is what we are trying to solve for in the end. $n_{ij}$ is the number of samples in bin $j$ of simulation $i$, and $N_i$ is the total number of samples in simulation $i$, such that $\\sum_j n_{ij} = N_i$. $c_{ij}$ is the Boltzmann factor of the bias in bin $j$ of simulation $i$, and therefore could be re-written as $e^{-\\frac{V_{ij}}{k_BT}}$. $f_i$ is nothing more than a normalization factor for simulation $i$, which ensures that the sum over every histogram is equal to $1$.\n",
    "\n",
    "Clearly, these equations take some time to digest, and in my experience, the only way to really understand them is to play around with them a bit. That said, there is a nice interpretation of the equation of $p_j^\\circ$. Imagine if we simply had $p_j^\\circ = \\frac{\\sum_{i=1}^{S}n_{ij}}{\\sum_{i=1}^{S}N_i}$. This would say that the probability in bin $j$ is equal to the total samples in bin $j$ across all $S$ simulations divided by the total samples across all bins across every simulation. This doesn't work, because every simulation $i$ has a different free energy profile due to the bias we apply, which is why the $c_{ij}$ term is necessary. The presence of $c_{ij}$, however, makes it so that the total probability across all bins of each simulation does not add up to $1$, which is why the $F_i$ normalization term must be present.\n",
    "\n",
    "In practice, to solve the WHAM equations, we give an initial guess for $f_i$ (usually just a string of ones), and then solve for $p_j^\\circ$. We plug that value of $p_j^\\circ$ into the first equation and solve again for $f_i$, and then continue going back and forth until both terms converge.\n",
    "\n",
    "Let's start by running a simulation with multiple windows. The following cell will run 10 simulations with evenly spaced harmonic bias potentials and plot the distributions that were sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c1d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run dynamics\n",
    "multi_umb = Umbrella_sampling(numbrellas=10,kappa=500,fes_func=deep_fes)\n",
    "multi_umb.dynamics(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results from previous cell.\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "x_vals = multi_umb.replicas[0].x_vals\n",
    "fes = deep_fes(x_vals)\n",
    "fes = fes - fes.min()\n",
    "prob_dist = np.exp(-fes)\n",
    "prob_dist /= np.sum(prob_dist*0.02)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(x_vals,fes,ls='dashed',label='Underlying FES')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(x_vals, prob_dist, ls='dashed', label='Underlying Probability')\n",
    "\n",
    "for i in range(multi_umb.numbrellas):\n",
    "    ax1.plot(x_vals,multi_umb.replicas[i].bias + fes)\n",
    "    hist, _ = np.histogram(multi_umb.replicas[i].trajectory,range=(-1,1),bins=100)\n",
    "    hist = hist/np.sum(hist*0.02)\n",
    "    ax2.plot(x_vals,hist)\n",
    "\n",
    "ax1.set_ylim(0,40)\n",
    "ax1.set_xlabel('CV')\n",
    "ax1.set_ylabel('Free Energy')\n",
    "ax1.set_title('Bias in Each Window')\n",
    "\n",
    "ax2.set_xlabel('CV')\n",
    "ax2.set_ylabel('Probability')\n",
    "ax2.set_title('Sampled Distribution in Each Simulation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e5563",
   "metadata": {},
   "source": [
    "The sampled results allow us to compute $n_{ij}$ and $N_i$ for WHAM. Let's analyze our trajectories with WHAM now, and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80345912",
   "metadata": {},
   "outputs": [],
   "source": [
    "wham = WHAM(multi_umb,iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966abbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results from previous cell.\n",
    "\n",
    "x_vals = multi_umb.replicas[0].x_vals\n",
    "fes = deep_fes(x_vals)\n",
    "plt.plot(x_vals,fes-fes.min(), label='True FES',lw=5)\n",
    "plt.plot(x_vals,wham.G-wham.G.min(),ls='dashed',label='WHAM Estimate',lw=3)\n",
    "plt.xlabel('CV')\n",
    "plt.ylabel('Free Energy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d1171",
   "metadata": {},
   "source": [
    "You can see how nicely we've been able to estimate the underlying free energy over (almost) all of CV space. (The sampling we missed on the edges could be corrected by placing bias potentials centered on the edges). The truth, however, is that we were only so successful because we selected an appropriate number of \"umbrellas\" with the correct values for kappa. Furthermore, we ran WHAM for 1000 iterations, more than enough for $f_i$ and $p_j^\\circ$ to converge. Below, you can run the simulation again but this time try adjusting these parameters, as well as the total number of steps in the simulation. Then, answer the following questions:\n",
    "\n",
    "1. What happens when there are too few umbrellas? What happens when there are too many?\n",
    "\n",
    "2. What happens when kappa is too low? Too high? \n",
    "\n",
    "3. What happens when there are too few steps in our simulation?\n",
    "\n",
    "4. How quickly do the WHAM results converge? How could we automatically detect if the results have converged?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb4a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Simulation Parameters { display-mode: \"form\" }\n",
    "numbrellas = 10 # @param {type:\"integer\"}\n",
    "kappa = 500 # @param {type:\"number\"}\n",
    "nsteps_per_umbrella = 1000000 # @param {type:\"integer\"}\n",
    "wham_iterations = 1000 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run simulation\n",
    "\n",
    "multi_umb = Umbrella_sampling(numbrellas=numbrellas,kappa=kappa,fes_func=deep_fes)\n",
    "multi_umb.dynamics(nsteps_per_umbrella)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "x_vals = multi_umb.replicas[0].x_vals\n",
    "fes = deep_fes(x_vals)\n",
    "fes = fes - fes.min()\n",
    "prob_dist = np.exp(-fes)\n",
    "prob_dist /= np.sum(prob_dist*0.02)\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.plot(x_vals,fes,ls='dashed',label='Underlying FES')\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.plot(x_vals, prob_dist, ls='dashed', label='Underlying Probability')\n",
    "\n",
    "for i in range(multi_umb.numbrellas):\n",
    "    ax1.plot(x_vals,multi_umb.replicas[i].bias + fes)\n",
    "    hist, _ = np.histogram(multi_umb.replicas[i].trajectory,range=(-1,1),bins=100)\n",
    "    hist = hist/np.sum(hist*0.02)\n",
    "    ax2.plot(x_vals,hist)\n",
    "\n",
    "ax1.set_ylim(0,40)\n",
    "ax1.set_xlabel('CV')\n",
    "ax1.set_ylabel('Free Energy')\n",
    "ax1.set_title('Bias in Each Window')\n",
    "\n",
    "ax2.set_xlabel('CV')\n",
    "ax2.set_ylabel('Probability')\n",
    "ax2.set_title('Sampled Distribution in Each Simulation')\n",
    "\n",
    "wham = WHAM(multi_umb,iterations=wham_iterations)\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.plot(x_vals,fes,label='True FES',lw=5)\n",
    "ax3.plot(x_vals,wham.G-wham.G.min(),ls='dashed',label='WHAM Estimate',lw=3)\n",
    "ax3.set_xlabel('CV')\n",
    "ax3.set_ylabel('Free Energy')\n",
    "ax3.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c8a65",
   "metadata": {},
   "source": [
    "## Appendix I: Free energy and probability.\n",
    "\n",
    "In an isolated system, that is, one at constant energy, all microstates $i$ of the system are equally likely.\n",
    "\n",
    "$$P_i = \\frac{1}{\\Omega(E)}$$\n",
    "\n",
    "Where $\\Omega(E)$ is the total number of states at energy $E$.\n",
    "\n",
    "Now, imagine we introduce an infinitely large bath of Temperature T, and bring it into contact with our system. Energy within the smaller system is no longer conserved, but the energy of the system plus the bath still is.\n",
    "\n",
    "$$E = E_{s} + E_{b}$$\n",
    "\n",
    "$$E_{b} = E - E_{s}$$\n",
    "\n",
    "Now we can describe the number of microstates of the bath in terms of the energy of the inner system.\n",
    "\n",
    "$$\\Omega_{b}(E_{b}) = \\Omega_{b}(E - E_{s})$$\n",
    "\n",
    "The probability of a microstate $i$ of the inner system occurring should be proportional to the number of possible microstates of the bath when the inner-system has an energy of $E_{s,i}$.\n",
    "\n",
    "$$P_i \\propto \\Omega_{b,i}(E - E_{s,i})$$\n",
    "\n",
    "This proportionality becomes an equality if we divide by the total number of microstates possible at any energy.\n",
    "\n",
    "$$P_i = \\frac{\\Omega_{b,i}(E - E_{s,i})}{\\sum_i \\Omega_{b,i}(E - E_{s,i})}$$\n",
    "\n",
    "We really need some idea of the functional form of $\\Omega_{b,i}(E - E_{s,i})$ for this to be useful to us, so we take the Taylor series expansion. Given that we care about a limit in which our bath is infinitely large, we can assume that $E - E_{s,i} \\approx E$, and thus we choose to center our expansion around energy $E$. Furthermore, it is known *a posteriori* that $\\Omega$ varies wildly with changing energy, so it is more useful to expand $\\ln \\Omega(E - E_{s,i})$.\n",
    "\n",
    "$$\\ln \\Omega_{b,i}(E - E_{s,i}) \\approx \\ln \\Omega_{b,i}(E) - E_{s,i} \\frac{\\mathrm{d} \\ln \\Omega_{b,i}(E - E_{s,i})}{\\mathrm{d} (E - E_{s,i})} \\Big|_{E - E_{s,i} = E}$$\n",
    "\n",
    "Within this first-order Taylor expansion we recognize something of the form $\\frac{\\mathrm{d} \\ln \\Omega(E)}{\\mathrm{d} (E)}$. In statistical mechanics, it is a well known identity that $\\beta = \\frac{1}{k_BT} = \\frac{\\mathrm{d} \\ln \\Omega(E)}{\\mathrm{d} (E)}$. We will utilize this identity to simplify the equation above.\n",
    "\n",
    "$$\\ln \\Omega_{b,i}(E - E_{s,i}) \\approx \\ln \\Omega_{b,i}(E) - \\frac{E_{s,i}}{k_BT}$$\n",
    "\n",
    "$$\\Omega_{b,i}(E - E_{s,i}) \\approx \\Omega_{b,i}(E) e^{-\\frac{E_{s,i}}{k_BT}}$$\n",
    "\n",
    "Now we can plug this form for $\\Omega$ back into our equation for $P_i$, recognizing that the constant term $\\Omega_{b,i}(E)$ will disappear because it is in both the numerator and every term in the denominator.\n",
    "\n",
    "$$P_i = \\frac{e^{-\\frac{E_{s,i}}{k_BT}}}{\\sum_j e^{-\\frac{E_{s,j}}{k_BT}}}$$\n",
    "\n",
    "We can see how the probability of a microstate depends on the energy of the system. We now need to justify replacing the internal energy $E$ in this equation with the free energy $F$. We know from thermodynamics that the way to go from internal energy to free energy is to add a dependence upon the entropy.\n",
    "\n",
    "$$F = E - TS$$\n",
    "\n",
    "So we will utilize another well-known result of statistical mechanics that relates the entropy to the density of states.\n",
    "\n",
    "$$S = k_B \\ln\\Omega$$\n",
    "\n",
    "Rearranging, we get\n",
    "\n",
    "$$\\Omega = e^{\\frac{S}{k_B}}$$\n",
    "\n",
    "Now we multiply $P_i$ by $e^{\\frac{S}{k_B}}$ on the top and the bottom.\n",
    "\n",
    "$$P_i = \\frac{e^{-\\frac{E_{s,i}}{k_BT}}}{\\sum_j e^{-\\frac{E_{s,j}}{k_BT}}} \\frac{e^{\\frac{S}{k_B}}}{e^{\\frac{S}{k_B}}}$$\n",
    "\n",
    "$$e^{-\\frac{E_{s,i}}{k_BT}}e^{\\frac{S}{k_B}} = e^{-\\frac{E_{s,i}}{k_BT} + \\frac{S}{k_B}} = e^{-\\frac{E_{s,i} + TS}{k_BT}} = e^{\\frac{E_{s,i} - TS}{k_BT}} = e^{\\frac{F_{s,i}}{k_BT}}$$\n",
    "\n",
    "$$P_i = \\frac{e^{-\\frac{F_{s,i}}{k_BT}}}{\\sum_j e^{-\\frac{F_{s,j}}{k_BT}}}$$\n",
    "\n",
    "Now we solve for $F_{s,i}$:\n",
    "\n",
    "$$P_i\\sum_j e^{-\\frac{F_{s,j}}{k_BT}} = e^{-\\frac{F_{s,i}}{k_BT}}$$\n",
    "\n",
    "$$\\ln \\bigg(P_i\\sum_j e^{-\\frac{F_{s,j}}{k_BT}}\\bigg) = -\\frac{F_{s,i}}{k_BT}$$\n",
    "\n",
    "$$-k_BT \\ln \\bigg(P_i\\bigg) - k_BT \\ln \\bigg(\\sum_j e^{-\\frac{F_{s,j}}{k_BT}}\\bigg) = F_{s,i}$$\n",
    "\n",
    "The partition function term ends up as nothing more than a constant shift. Because we are only interested in free energy differences, and not absolute free energies, this term is unimportant and is usually omitted to give us our final equation:\n",
    "\n",
    "$$F_i = -k_BT \\ln (P_i)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
